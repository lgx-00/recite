{
    "test": [["as{}d", "123"]],
    "大数据技术概论": [
      [
        "bitmap.svg",
        {"#Zookeeper": "分布式协作服务", "#Hive": "数据仓库", "#MapReduce": "分布式计算框架", "#YARN": "资源调度和管理框架", "#HDFS": "分布式文件系统"}
      ], [
        "Hadoop 中的 MapReduce 计算框架存在以下问题:<br>(1){}。计算都必须转化成 Map 和 Reduce 两个操作，但这并不适合所有的情况，难以描述复杂的数据处理过程。<br>(2){}。每次执行时都需要从磁盘中读取数据，并且在计算完成后需要将中间结果写入磁盘中，I/O 开销较大。<br>(3){}。一次计算可能需要分解成一系列按顺序执行的 Map 和 Reduce 任务，任务之间的衔接由于涉及 I/O 开销，会产生较高延迟。而且，在前一个任务指向完成之前，后面的任务无法开始，因此，其难以胜任复杂、多程序的计算任务。<br>(4){}。虽然 MapReduce 能够处理大规模的通用批处理作业，但结合机器学习、流处理或者交互式 SQL 查询等其他应用场景的话，难免“力不从心”。为了应对这些新场景，工程师开发了一些定制化的系统，如 Hive、Storm、Impala、Giraph、Drill 和 Mahout 等，但是，这些系统都有自己的 API 和集群配置选项，因而进一步增加了 Hadoop 集群的韵味复杂度也使 Hadoop 开发的学习曲线更加陡峭。(P7)",
        "表达能力有限", "磁盘 I/O 开销大", "延迟高", "难以适用于多种应用场景"
      ], [
        "相比于 MapReduce , Spark 主要具有如下优点：<br>(1){}。<br>(2){}。<br>(3){}。<br>(4){}。(P7)",
        "Spark 的计算框架类似于 MapReduce， 但不限于 Map 和 Reduce 操作，还提供了多种数据集操作类型，编程模型比 MapReduce 更灵活",
        "Spark 提供了内存计算，中间结果直接放到内存中，带来了更高的迭代运算效率", "Spark 基于 DAG 的任务调度执行机制，要优于 MapReduce 的迭代执行机制",
        "Spark 提供了多种组件，可以一站式支持批处理、流处理、查询分析、图计算、机械学习等不同应用场景"
      ]  
    ],
    "Scala 语言基础": [
      [
        "什么是字面量：{}。<br>如: val b='A'，'A'就是 val b 的字面量。function A={}，{}里包含的内容是 function A 的字面量(P23)",
        "字面量是直接在源代码里书写常量值的一种方式"
      ], [
        "Scala 使用{}结构来捕获异常。形式如:<br>{} {<br>&nbsp;&nbsp;&nbsp;&nbsp;// 代码块<br>} {} {<br>&nbsp;&nbsp;&nbsp;&nbsp;{}: Exception => 代码块<br>} {} {<br>&nbsp;&nbsp;&nbsp;&nbsp;代码块<br> }&nbsp;(P29)",
        "try-catch", "try", "catch", "case ex", "finally"
      ], [
        "Scala 中通过一个名称为{}的类来实现对循环的控制，该类位于包{}下。该类有两个方法用于对循环结构的控制即{}和{}。(P29)",
        "Breaks", "scala.util.control", "breakable", "break"
      ], [
        "在 scala 编程中经常需要用到各种数据结构，如{}、{}、{}、{}、{}、{}、{}等(P30-P34)",
        "数组", "元组", "容器", "序列", "集合", "映射", "迭代器"
      ], [
        "2.3 面向对象编程<br>2.3.1 类(P35-P39), 2.3.2 对象(P40), 2.3.3 继承(P44-P47), 2.3.5 特质(P49-P52){}",
        ""
      ], [
        "Scala 的类使用关键字{}声明。最简单的类的定义形式如下<br>{} { <br>// {} <br> }",
        "class", "class 类名", "类的字段和方法"
      ], [
        "类的构造器: <br>1. 共有多少种构造器。{}种，它们分别为{}和{}。<br>2. 是否可以只有辅助构造器而没有主构造器。{}(请填写 是/否)<br>3. 如果有多个辅助构造器，构造顺序是什么。{}",
        "2", "主构造器", "辅助构造器", "否", "先调用主构造器，然后才调用辅助构造器。如果一个辅助构造器调用了另一个辅助构造器，那么被调用的辅助构造器会在调用者构造器之前被调用"
      ], [
        "1. scala 语言是单继承还是多继承。{}。<br>2. scala 中是否有接口。{}。",
        "单继承", "没有"
      ], [
        "高阶函数的定义: {}。<br>高阶函数的作用: {}。(P57)(重点看)",
        "当一个函数以其他函数作为其参数或者返回结果为一个函数时，该函数被称为高阶函数", "高阶函数可以将灵活、细粒度的代码块集合成更大、更复杂的程序"
      ], [
        "2.4.5 针对容器的操作(P60-P64)"
      ]
    ],
    "Spark 的设计与运行原理": [
      [
        "spark 每个组件的用处: <br>(1) Spark Core: {}。<br>(2) Spark SQL: {}。<br>(3) Spark Streaming: {}。<br>(4) Structured Streaming: {}。<br>(5) MLlib(机器学习): {}。<br>(6) GraphX(图计算): {}。(P73)",
        "Spark Core 包含 spark 最基础和最核心的功能，如内存计算、任务调度、部署模式、故障恢复、存储管理等，主要面向批量数据处理",
        "Spark SQL 是用于处理结构化数据的组件。允许开发人员直接处理 RDD 同时也可查询 Hive，Hbase 等外部数据源",
        "Spark Streaming 是一种流计算框架，可以支持高吞吐量、可容错的实时流数据处理",
        "Structured Streaming 是一种基于 Spark SQL 引擎构建的可扩展且可容错的流处理引擎",
        "MLlib 提供了常用机器算法的实现",
        "GraphX 是 Spark 中用于图计算的 API，能在海量数据上自如地运行复杂的图算法"
      ], [
        "图 3-2 Spark 运行框架(P75)",
        ""
      ], [
        "Spark 是{}级的并行，不是{}级的并行。",
        "线程", "进程"
      ], [
        "RDD 概念: {}。(P77)",
        "一个 RDD 就是一个分布式对象集合，本质上是一个只读的分区记录集合"
      ], [
        "RDD 特性: <br>(1){}。<br>(2) {}。数据在内存中的多个 RDD 之间进行传递，不需要“落地”到磁盘上，避免了不必要的读写磁盘开销。<br>(3) {}。避免了不必要的对象序列化和反序列化开销。(P79)",
        "高效的容错性", "中间结果持久化到内存", "存放的数据可以是 Java 对象。"
      ], [
        "RDD 之间的依赖关系分为{}和{}，二者的主要区别在于{}。(P79-P83)",
        "窄依赖", "宽依赖", "是否包含 Shuffle 过程"
      ]
    ],
    "Spark 环境搭建和使用方法": [
      [
        "为了能够使用 sbt 的 Spark 应用程序进行编译打包，我们需要把应用程序代码存放在应用程序根目录下的{}目录下。(P94)",
        "src/main/scala"
      ], [
        "在应用程序根目录下的扩展名为 .sbt 文件中双`%`号和单`%`号的区别在于。<br>{}(P95)",
        "双`%`会根据当前项目所使用的 Scala 版本自动选择相应的依赖版本，而单`%`则不会根据 Scala 版本进行自动选择，需要明确指定依赖的版本号"
      ]
    ],
    "RDD 编程基础": [
      [
      ]
    ]
  }